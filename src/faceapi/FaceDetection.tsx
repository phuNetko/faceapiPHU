import React, { useEffect, useRef } from "react";
import * as faceapi from "@vladmandic/face-api";

const loadModels = async () => {
  try {
    console.log("â³ Äang táº£i models...");
    await faceapi.nets.tinyFaceDetector.loadFromUri("/models").catch((error) => {
      console.error("âŒ Lá»—i khi táº£i tinyFaceDetector:", error);
    });
    await faceapi.nets.ssdMobilenetv1.loadFromUri("/models").catch((error) => {
      console.error("âŒ Lá»—i khi táº£i ssdMobilenetv1:", error);
    });
    await faceapi.nets.faceLandmark68Net.loadFromUri("/models").catch((error) => {
      console.error("âŒ Lá»—i khi táº£i faceLandmark68Net:", error);
    });
    await faceapi.nets.faceRecognitionNet.loadFromUri("/models").catch((error) => {
      console.error("âŒ Lá»—i khi táº£i faceRecognitionNet:", error);
    });
    console.log("âœ… Models loaded successfully!");
  } catch (error) {
    console.error("âŒ Lá»—i khi táº£i models:", error);
  }
};
const FaceDetection: React.FC = () => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);

  useEffect(() => {
    const startVideo = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
        if (videoRef.current) videoRef.current.srcObject = stream;
      } catch (error) {
        console.error("âŒ Lá»—i khi má»Ÿ camera:", error);
      }
    };

    const detectFaces = async () => {
      if (!videoRef.current || !canvasRef.current) return;
      
      const video = videoRef.current;
      const canvas = canvasRef.current;

      video.addEventListener("loadeddata", async () => {
        console.log("ðŸ“¸ Camera Ä‘Ã£ sáºµn sÃ ng!");

        const displaySize = { width: video.videoWidth, height: video.videoHeight+50 };
        faceapi.matchDimensions(canvas, displaySize);

        setInterval(async () => {
          const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions());
          const resizedDetections = faceapi.resizeResults(detections, displaySize);
// console.log(resizedDetections);

          const ctx = canvas.getContext("2d");
          if (ctx) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            // faceapi.draw.drawDetections(canvas, resizedDetections);
            resizedDetections.forEach((detection) => {
              const { x, y, width, height } = detection.box;
          const cornerSize = 30;
              // ðŸŒŸ Váº½ khung nhÆ°ng KHÃ”NG hiá»ƒn thá»‹ sá»‘
              ctx.strokeStyle = "rgba(52, 173, 218, 0.8)";
              ctx.lineWidth = 3;
              ctx.shadowColor = "rgba(209, 220, 209, 0.5)";
              ctx.shadowBlur = 10;
          
              ctx.beginPath();
              // ctx.roundRect(x, y-100, width, height+50, [15]);
   // GÃ³c trÃªn trÃ¡i
ctx.moveTo(x, y + cornerSize - 50);
ctx.lineTo(x, y - 50);
ctx.lineTo(x + cornerSize, y - 50);

// GÃ³c trÃªn pháº£i
ctx.moveTo(x + width - cornerSize, y - 50);
ctx.lineTo(x + width, y - 50);
ctx.lineTo(x + width, y + cornerSize - 50);

// GÃ³c dÆ°á»›i pháº£i
ctx.moveTo(x + width, y + height - cornerSize - 50);
ctx.lineTo(x + width, y + height - 50);
ctx.lineTo(x + width - cornerSize, y + height - 50);

// GÃ³c dÆ°á»›i trÃ¡i
ctx.moveTo(x + cornerSize, y + height - 50);
ctx.lineTo(x, y + height - 50);
ctx.lineTo(x, y + height - cornerSize - 50);

              ctx.stroke();
              ctx.closePath();
            });
            
          }
        }, 100);
      });
    };

    loadModels().then(() => {
      startVideo().then(detectFaces);
    });
  }, []);

  return (
    <div className="relative">
      <video ref={videoRef} autoPlay muted className="absolute top-0 left-0 w-full h-auto" />
      <canvas ref={canvasRef} className="absolute top-0 left-0 w-full h-auto" />
    </div>
  );
};

export default FaceDetection;
